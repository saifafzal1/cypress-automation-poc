pipeline {
    agent any

    environment {
        CYPRESS_BASE_URL = 'https://katalon-demo-cura.herokuapp.com'
    }

    options {
        timestamps()
        timeout(time: 120, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }

    stages {

        // ── Stage 1: Checkout ──
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        // ── Stage 2: Prepare ──
        stage('Prepare') {
            steps {
                sh '''
                    rm -rf benchmark-results benchmark-report
                    mkdir -p benchmark-results benchmark-report
                '''
            }
        }

        // ── Benchmark Stages: One per Cypress version ──

        stage('Cypress 9.7.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('9.7.0')
                }
            }
        }

        stage('Cypress 10.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('10.0.0')
                }
            }
        }

        stage('Cypress 11.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('11.0.0')
                }
            }
        }

        stage('Cypress 12.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('12.0.0')
                }
            }
        }

        stage('Cypress 13.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('13.0.0')
                }
            }
        }

        stage('Cypress 13.6.6') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('13.6.6')
                }
            }
        }

        stage('Cypress 14.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('14.0.0')
                }
            }
        }

        stage('Cypress 15.3.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('15.3.0')
                }
            }
        }

        // ── Generate Benchmark Report ──
        stage('Generate Report') {
            steps {
                sh '''
                    echo "=== Collected metrics files ==="
                    ls -la benchmark-results/
                    echo "=== Contents of metric files ==="
                    cat benchmark-results/*.metric 2>/dev/null || echo "No metric files found"
                    echo "==============================="

                    echo "Generating benchmark comparison report..."
                    CONTAINER_ID=$(docker create --entrypoint="" node:20-alpine sh -c "mkdir -p /app/benchmark-report && node /app/scripts/generate-benchmark-report.js")
                    docker cp ${WORKSPACE}/scripts ${CONTAINER_ID}:/app/scripts
                    docker cp ${WORKSPACE}/benchmark-results ${CONTAINER_ID}:/app/benchmark-results
                    docker start -a ${CONTAINER_ID}
                    docker cp ${CONTAINER_ID}:/app/benchmark-report/index.html ${WORKSPACE}/benchmark-report/index.html
                    docker rm ${CONTAINER_ID}
                '''
            }
        }

        // ── Publish Report ──
        stage('Publish Report') {
            steps {
                publishHTML(target: [
                    allowMissing: true,
                    alwaysLinkToLastBuild: true,
                    keepAll: true,
                    reportDir: 'benchmark-report',
                    reportFiles: 'index.html',
                    reportName: 'Cypress Version Benchmark',
                    reportTitles: 'Version Performance Comparison'
                ])
            }
        }

    } // end stages

    post {
        always {
            archiveArtifacts artifacts: 'benchmark-results/**', allowEmptyArchive: true
            archiveArtifacts artifacts: 'benchmark-report/**', allowEmptyArchive: true
        }
        success {
            echo 'Benchmark completed successfully!'
        }
        cleanup {
            cleanWs()
        }
    }
}

// ── Shared function: Run benchmark for a specific Cypress version ──
def runBenchmark(String cypressVersion) {
    def containerName = "cypress-bench-${cypressVersion.replace('.', '-')}"

    sh """
        echo "========================================"
        echo "  Benchmarking Cypress ${cypressVersion}"
        echo "========================================"

        # Record start time
        START_TIME=\$(date +%s%N)

        # Create container - run Cypress and save output log
        docker create --name ${containerName} \
            -e CYPRESS_BASE_URL=${CYPRESS_BASE_URL} \
            -w /app \
            --entrypoint="" \
            cypress/included:${cypressVersion} \
            sh -c "npx cypress run --spec 'cypress/e2e/smoke/**' 2>&1 | tee /app/cypress-output.log; exit 0"

        # Copy test files into container
        docker cp \${WORKSPACE}/cypress ${containerName}:/app/cypress
        docker cp \${WORKSPACE}/cypress.config.js ${containerName}:/app/cypress.config.js

        # Run the container
        docker start -a ${containerName} || true

        # Record end time
        END_TIME=\$(date +%s%N)
        TOTAL_TIME_MS=\$(( (END_TIME - START_TIME) / 1000000 ))
        echo "Container total time: \${TOTAL_TIME_MS}ms"

        # Copy the Cypress output log from the container
        docker cp ${containerName}:/app/cypress-output.log \${WORKSPACE}/benchmark-results/cypress-${cypressVersion}.log 2>/dev/null || echo "WARNING: Could not copy output log"

        # Cleanup container
        docker rm -f ${containerName} 2>/dev/null || true

        # Write a metric file for this version (simple key=value format)
        METRIC_FILE=\${WORKSPACE}/benchmark-results/${cypressVersion}.metric
        echo "version=${cypressVersion}" > \${METRIC_FILE}
        echo "totalTime=\${TOTAL_TIME_MS}" >> \${METRIC_FILE}

        LOG_FILE=\${WORKSPACE}/benchmark-results/cypress-${cypressVersion}.log
        if [ -f "\${LOG_FILE}" ]; then
            echo "=== Log file size: \$(wc -c < "\${LOG_FILE}") bytes ==="

            # Strip ANSI codes for parsing
            CLEAN_LOG=\${WORKSPACE}/benchmark-results/cypress-${cypressVersion}.clean.log
            sed 's/\\x1b\\[[0-9;]*m//g' "\${LOG_FILE}" > "\${CLEAN_LOG}"

            # Look for the spec result table rows
            # Each spec: "✔  filename.cy.js  MM:SS  tests  passes  failures  pending  skipped"
            # Final:     "✔  All specs passed!  MM:SS  tests  passes  failures  pending  skipped"
            # Or:        "✖  N of N failed"

            # Count passing checkmarks (✓ or ✔) for individual test cases
            PASS_MARKS=\$(grep -c '✓' "\${LOG_FILE}" 2>/dev/null || echo "0")
            FAIL_MARKS=\$(grep -c 'failing' "\${LOG_FILE}" 2>/dev/null || echo "0")
            echo "passMarks=\${PASS_MARKS}" >> \${METRIC_FILE}
            echo "failMarks=\${FAIL_MARKS}" >> \${METRIC_FILE}

            # Extract total duration from "All specs passed! MM:SS" or similar
            DURATION_LINE=\$(grep -E '(All specs passed|specs? failed)' "\${CLEAN_LOG}" | tail -1 || true)
            echo "durationLine=\${DURATION_LINE}" >> \${METRIC_FILE}

            # Grep the results summary table for numbers
            # Try to find the bottom summary row that has totals
            echo "=== Spec result lines ==="
            grep -E '\\.(cy|spec)\\.' "\${CLEAN_LOG}" | grep -E '[0-9]+:[0-9]+' || true
            echo "=== Summary line ==="
            echo "\${DURATION_LINE}"
        else
            echo "WARNING: No log file found for ${cypressVersion}"
            echo "passMarks=0" >> \${METRIC_FILE}
            echo "failMarks=0" >> \${METRIC_FILE}
        fi

        echo "Metric file written: \${METRIC_FILE}"
        cat \${METRIC_FILE}
    """
}
