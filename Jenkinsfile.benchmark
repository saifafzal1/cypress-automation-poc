pipeline {
    agent any

    environment {
        CYPRESS_BASE_URL = 'https://katalon-demo-cura.herokuapp.com'
    }

    options {
        timestamps()
        timeout(time: 120, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }

    stages {

        // ── Stage 1: Checkout ──
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        // ── Stage 2: Prepare ──
        stage('Prepare') {
            steps {
                sh '''
                    rm -rf benchmark-results benchmark-report
                    mkdir -p benchmark-results benchmark-report
                '''
            }
        }

        // ── Benchmark Stages: One per Cypress version ──

        stage('Cypress 9.7.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('9.7.0')
                }
            }
        }

        stage('Cypress 10.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('10.0.0')
                }
            }
        }

        stage('Cypress 11.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('11.0.0')
                }
            }
        }

        stage('Cypress 12.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('12.0.0')
                }
            }
        }

        stage('Cypress 13.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('13.0.0')
                }
            }
        }

        stage('Cypress 13.6.6') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('13.6.6')
                }
            }
        }

        stage('Cypress 14.0.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('14.0.0')
                }
            }
        }

        stage('Cypress 15.3.0') {
            steps {
                catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                    runBenchmark('15.3.0')
                }
            }
        }

        // ── Generate Benchmark Report ──
        stage('Generate Report') {
            steps {
                sh '''
                    echo "=== Collected benchmark files ==="
                    ls -la benchmark-results/
                    echo ""
                    for f in benchmark-results/*.metric; do
                        echo "--- $f ---"
                        cat "$f"
                        echo ""
                    done
                    echo "==============================="

                    echo "Generating benchmark comparison report..."
                    # Start a long-running container so we can exec into it
                    CONTAINER_ID=$(docker run -d --entrypoint="" node:20-alpine sleep 300)
                    docker exec ${CONTAINER_ID} mkdir -p /app/benchmark-report
                    docker cp ${WORKSPACE}/scripts ${CONTAINER_ID}:/app/scripts
                    docker cp ${WORKSPACE}/benchmark-results ${CONTAINER_ID}:/app/benchmark-results
                    docker exec ${CONTAINER_ID} node /app/scripts/generate-benchmark-report.js
                    docker cp ${CONTAINER_ID}:/app/benchmark-report/index.html ${WORKSPACE}/benchmark-report/index.html
                    docker rm -f ${CONTAINER_ID}
                '''
            }
        }

        // ── Publish Report ──
        stage('Publish Report') {
            steps {
                publishHTML(target: [
                    allowMissing: true,
                    alwaysLinkToLastBuild: true,
                    keepAll: true,
                    reportDir: 'benchmark-report',
                    reportFiles: 'index.html',
                    reportName: 'Cypress Version Benchmark',
                    reportTitles: 'Version Performance Comparison'
                ])
            }
        }

    } // end stages

    post {
        always {
            archiveArtifacts artifacts: 'benchmark-results/**', allowEmptyArchive: true
            archiveArtifacts artifacts: 'benchmark-report/**', allowEmptyArchive: true
        }
        success {
            echo 'Benchmark completed successfully!'
        }
        cleanup {
            cleanWs()
        }
    }
}

// ── Shared function: Run benchmark for a specific Cypress version ──
def runBenchmark(String cypressVersion) {
    def containerName = "cypress-bench-${cypressVersion.replace('.', '-')}"
    def logFile = "benchmark-results/cypress-${cypressVersion}.log"

    sh """
        echo "========================================"
        echo "  Benchmarking Cypress ${cypressVersion}"
        echo "========================================"

        # Record start time
        START_TIME=\$(date +%s%N)

        # Create container - run benchmark script (handles dependency fixes)
        docker create --name ${containerName} \
            -e CYPRESS_BASE_URL=${CYPRESS_BASE_URL} \
            -w /app \
            --entrypoint="" \
            cypress/included:${cypressVersion} \
            sh /app/run-benchmark.sh

        # Copy test files and runner script into container
        docker cp \${WORKSPACE}/cypress ${containerName}:/app/cypress
        docker cp \${WORKSPACE}/scripts/run-benchmark.sh ${containerName}:/app/run-benchmark.sh

        # Cypress 9.x uses cypress.json; 10+ uses cypress.config.js
        MAJOR=\$(echo "${cypressVersion}" | cut -d. -f1)
        if [ "\${MAJOR}" -lt 10 ]; then
            docker cp \${WORKSPACE}/cypress.json ${containerName}:/app/cypress.json
        else
            docker cp \${WORKSPACE}/cypress.config.benchmark.js ${containerName}:/app/cypress.config.js
        fi

        # Run the container — capture output with tee on the HOST side
        docker start -a ${containerName} 2>&1 | tee \${WORKSPACE}/${logFile} || true

        # Record end time
        END_TIME=\$(date +%s%N)
        TOTAL_TIME_MS=\$(( (END_TIME - START_TIME) / 1000000 ))
        echo "Container total time: \${TOTAL_TIME_MS}ms"

        # Cleanup container
        docker rm -f ${containerName} 2>/dev/null || true

        # Write metric file
        METRIC_FILE=\${WORKSPACE}/benchmark-results/${cypressVersion}.metric
        echo "version=${cypressVersion}" > \${METRIC_FILE}
        echo "totalTime=\${TOTAL_TIME_MS}" >> \${METRIC_FILE}

        LOG=\${WORKSPACE}/${logFile}
        PASSES=0
        FAILURES=0
        TESTS=0
        DURATION_MS=0

        if [ -f "\${LOG}" ]; then
            LOG_SIZE=\$(wc -c < "\${LOG}")
            echo "Log captured: \${LOG_SIZE} bytes"

            # Strip ANSI codes for parsing
            CLEAN=\${WORKSPACE}/benchmark-results/cypress-${cypressVersion}.clean.log
            sed 's/\\x1b\\[[0-9;]*m//g; s/\\x1b\\[K//g' "\${LOG}" > "\${CLEAN}" || true

            # Parse "N passing" from Cypress spec reporter output
            PASS_LINE=\$(grep -oE '[0-9]+ passing' "\${CLEAN}" | tail -1) || true
            if [ -n "\${PASS_LINE}" ]; then
                PASSES=\$(echo "\${PASS_LINE}" | grep -oE '[0-9]+' | head -1) || true
            fi

            # Parse "N failing"
            FAIL_LINE=\$(grep -oE '[0-9]+ failing' "\${CLEAN}" | tail -1) || true
            if [ -n "\${FAIL_LINE}" ]; then
                FAILURES=\$(echo "\${FAIL_LINE}" | grep -oE '[0-9]+' | head -1) || true
            fi

            TESTS=\$(( PASSES + FAILURES ))

            # Extract duration from summary "All specs passed!  MM:SS" or per-spec durations
            SUMMARY=\$(grep 'All specs passed\\|specs.*failed' "\${CLEAN}" | tail -1) || true
            if [ -n "\${SUMMARY}" ]; then
                DUR_STR=\$(echo "\${SUMMARY}" | grep -oE '[0-9]+:[0-9]+' | tail -1) || true
                if [ -n "\${DUR_STR}" ]; then
                    MINS=\$(echo "\${DUR_STR}" | cut -d: -f1)
                    SECS=\$(echo "\${DUR_STR}" | cut -d: -f2)
                    DURATION_MS=\$(( (MINS * 60 + SECS) * 1000 ))
                fi
            fi

            echo "Parsed: tests=\${TESTS} passes=\${PASSES} failures=\${FAILURES} duration=\${DURATION_MS}ms"
        else
            echo "WARNING: No log file"
        fi

        echo "tests=\${TESTS}" >> \${METRIC_FILE}
        echo "passes=\${PASSES}" >> \${METRIC_FILE}
        echo "failures=\${FAILURES}" >> \${METRIC_FILE}
        echo "duration=\${DURATION_MS}" >> \${METRIC_FILE}

        echo "--- Metric ---"
        cat \${METRIC_FILE}
    """
}
